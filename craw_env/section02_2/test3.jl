{"contents": ["From inconsistent website layouts that break our extraction logic to badly written HTML, web scraping comes with its share of difficulties. Over the last few years, the single most important challenge in web scraping has been to actually get to the data - and not get blocked. This is due to the antibots or the underlying technologies that websites use to protect their data. Proxies are a major component in any scalable web scraping infrastructure. However, not many people understand the technicalities of the different types of proxies and how to make the best use of proxies to get the data they want, with the least possible blocks.", "Oftentimes the\u00a0emphasis is on proxies to get around antibots. But the logic of the scraper is important too. "]}
{"contents": ["In this article, we build upon some of the semi-automated techniques and tools introduced in the ", " of the series.", "Let\u2019s say that the data we work with is separated by comma and line breaks:", "However, there isn\u2019t a consistency of how many words are separated by comma per line. To make it easier, we can transform the data set so there\u2019s only one word in each:", "In order to make this transformation of the data we can use search and replace functionalities of code text editors such as "]}
{"contents": ["Article extraction is the process of extracting data fields from an article page and putting it into a machine-readable structured format like JSON. In many use cases, the article page that you want to extract is a news page but it can be any other type of article. Based on our experience in the web data extraction industry for over 10 years, the demand for structured article data is getting higher. There is more information available on the internet than ever. But still, having access to structured news data and being able to consume relevant and timely information can set you apart and give you a competitive edge. This is what article extraction can do for you.", "In order to extract article data from the web, you need an extraction tool. It can be a challenge to find the tool that is best suited to meet your needs and provides the functionality and data quality that you expect. In this article, we discuss the most used tools for article extraction that you can choose from.", "We\u2019re going to look at both open source and commercial solutions. Hopefully, by the end of this blog post, you will have a better understanding of the available article extraction tools and you will be able to make an educated decision whether an open-source or closed-source tool is the best for you to extract news and/or articles.", "Before looking at the tools, let\u2019s have a quick overview of what is included in article extraction and why you would extract articles.", "An article\u2019s most important data field and many times the reason you want to extract the data is the "]}
{"contents": ["Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions.", "However, as the number of websites continues to grow, data extraction has become increasingly complex. Constantly changing website structures, antibot practices and scaling requirements add to the difficulty.", "To overcome the ever-increasing challenges, the web data extraction industry has gone through impressive innovations and transformations in recent years that enable extracting accurate data in an efficient manner. During this revolutionary time in the industry, we are delighted to announce that Scrapinghub will be once again hosting the", " this year on ", "."]}
{"contents": ["During a broad crawl, you might be extracting data from thousands or tens of thousands of websites with different layouts. When you scrape this many websites using a single spider, analyzing and validating the extracted data can be challenging. One important question to answer is what criteria should be used to determine the overall quality of the dataset.", "In this article, which is the final part of our QA series, we\u2019re going to work with 20 000 different sites and go through all steps with detailed explanations about the process. In this example, we are interested in finding specific keywords on the sites - if the keyword is found on a page then it should be flagged for further analysis.", "A large subset of heterogeneous sites should be processed to collect information for a specific keyword. We are going to crawl recursively starting from the domain and check all found links on the landing page. Several types of limits can be introduced here, like:", "The goal is to get as many pages as possible and verify the existence of keywords in them.", "The first step is inspired by two popular topics in programming:"]}
{"contents": ["But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution.", " proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.", "Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.", "Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.", "Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean."]}
{"contents": ["Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.", "From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.", "During our webinar on \u201c", "\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:", "A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code."]}
{"contents": ["We are excited to announce our newest data extraction API. The ", " is now publicly available as a BETA release.", "If you want to skip the introductions and just get stuck in, here are the links you need:", "AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as ", " and more into the arena of blog comment analysis.\u00a0"]}
{"contents": ["Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.", "Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where ", "(HCF) comes to our rescue.", "HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to ", ", but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with "]}
{"contents": ["As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data.", "In this article, you will see the many ways real estate data can help you and how utilizing web scraping can help you and your organization become disruption-proofed and fully prepared for the world of tomorrow.", "There is more public data available in the real estate market than ever. There are numerous listing sites, endless data points available for everyone to see. And if there\u2019s data, there should be a way to ", " from the data to make better decisions. But there\u2019s one big problem...", "Unfortunately, many websites don\u2019t provide "]}
{"contents": ["From inconsistent website layouts that break our extraction logic to badly written HTML, web scraping comes with its share of difficulties. Over the last few years, the single most important challenge in web scraping has been to actually get to the data - and not get blocked. This is due to the antibots or the underlying technologies that websites use to protect their data. Proxies are a major component in any scalable web scraping infrastructure. However, not many people understand the technicalities of the different types of proxies and how to make the best use of proxies to get the data they want, with the least possible blocks.", "Oftentimes the\u00a0emphasis is on proxies to get around antibots. But the logic of the scraper is important too. "]}
{"contents": ["But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution.", " proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.", "Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.", "Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.", "Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean.", "The biggest issue with residential proxies is, as it was mentioned, they are expensive. So usually the most effective way to scale your web data extraction project, is to try to maximize the value of data center proxies, by being smart about how you actually scrape the web and how you use proxies.", "Two things, that you can do to achieve this:", "If you want to learn more about these tactics, I recommend watching our FREE webinar on this: ", "If you missed our webinar on the topic of data center proxies and residential proxies don\u2019t worry you will be able to watch it here:", "If you feel like you know enough already and you don't want to spend way too much time on managing proxies, you can just use an already existing solution for "]}
{"contents": ["Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.", "From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.", "During our webinar on \u201c", "\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:", "A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code.", "A: Websites showcase erroneous pricing data when they detect you scraping regularly. This especially happens when you are trying to scale - i.e scrape a lot of products very frequently. Erroneous pricing is not easily recognizable, but comparing the prices or other data fields with previously extracted data and manually checking if there is a big difference in the extracted data can help.", "The long-term solution for this would be to be smarter about how you scale and be more thoughtful about the ", " you use.", "A: Scraping accurate data is all about having a reliable quality assurance process. The first step towards this process is to have a well-defined JSON schema. Your QA process needs to be a balanced combination of automated ways of testing the data as well as manual ways. This blog post gives a detailed description of ", "."]}
{"contents": ["We are excited to announce our newest data extraction API. The ", " is now publicly available as a BETA release.", "If you want to skip the introductions and just get stuck in, here are the links you need:", "AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as ", " and more into the arena of blog comment analysis.\u00a0", "The underlying data model for the API was released to production as part of 20.6.0 release of AutoExtract.", "Customer support management presents many challenges due to the sheer number of requests, varied topics, and diverse departments within a company that might have a say in resolving the matter.", "Sourcing structured data from blog comments as provided by our API can be used in tandem with ", " solutions to quickly and effectively identify, track and act upon particular conversation strings \u2018hidden\u2019 amongst the noise of thousands of comments. You are effectively highlighting warning signs that your CX team should become involved before an incident takes place.", "Another particular powerful insight that can be derived from comments revolving around the sphere of Voice of Customer (VoC) and product analysis. By tapping into blog comments, you can search keywords for a particular product or feature or use the parsed data to train sentiment analysis model to find only the information you need."]}
{"contents": ["Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.", "Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where ", "(HCF) comes to our rescue.", "HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to ", ", but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with ", ", you may be wondering why one would use HCF, when Scrapy can store and recover the crawling state by itself.\u00a0", "The advantage is that Scrapy requires you to manage this state, by saving the content to disk (so needs disk quota) and if you are running inside a container, like in Scrapy Cloud, local files are lost once the process is finished. So, having some kind of external storage for requests is an alternative that takes this burden from your shoulders, leaving you to think about the extraction logic and not about the details on how to proceed in case it crashes and you need to restart.", "Before digging into an example of how to use HCF, I\u2019ll go over a bit on how it is structured. We can create many Frontiers per project, for each one we need a name. These Frontiers are then broken into slots, something similar to sharding, that can be useful in a producer-consumer scenario (topic of one of our upcoming blog posts). Usually, the name will be the name of the spider, to avoid any confusion. The catchy part is that we shouldn't change the number of slots after it was created, so keep it in mind when creating it.", "Now that we know what HCF is and how we could make use of it, it is time to see it working. For this purpose, we\u2019ll build a simple Scrapy spider to extract book information from ", ". To get started, we\u2019ll create a new scrapy project and install the proper dependencies as shown below (type them in your terminal)."]}
{"contents": ["As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data.", "In this article, you will see the many ways real estate data can help you and how utilizing web scraping can help you and your organization become disruption-proofed and fully prepared for the world of tomorrow.", "There is more public data available in the real estate market than ever. There are numerous listing sites, endless data points available for everyone to see. And if there\u2019s data, there should be a way to ", " from the data to make better decisions. But there\u2019s one big problem...", "Unfortunately, many websites don\u2019t provide ", ". Or even if they do, you might not get all the data you want only in a limited fashion. But still, the publicly available data is there, you just don\u2019t have a straightforward way to get the data. This is where web data extraction comes in. Web data extraction allows you to get this publicly available real estate data at scale. Using the correct tools or partnering with a ", ", like Scrapinghub, allows you to tap into the world of web scraping and enjoy the benefits of ", ".", "There are many situations where estimating the value of a property is necessary. Maybe you\u2019re trying to list it online for the most accurate price, maybe you\u2019re trying to get financing or you\u2019re analyzing a property before purchasing. You want to get the most accurate value of how much the property is worth.", "Being in the real estate market means that you have a lot of competition. In order to be ahead of the competition, you need to find ways to know more than others. As most realtors get their data from a single listing like the MLS, you can differentiate yourself by accessing alternative data sources. Web data extraction can help by allowing you to fetch structured "]}
{"contents": ["In this article, we build upon some of the semi-automated techniques and tools introduced in the ", " of the series.", "Let\u2019s say that the data we work with is separated by comma and line breaks:", "However, there isn\u2019t a consistency of how many words are separated by comma per line. To make it easier, we can transform the data set so there\u2019s only one word in each:", "In order to make this transformation of the data we can use search and replace functionalities of code text editors such as ", ", ", " or ", ".\u00a0", "This how you can do it using Sublime Text:", "Once the process finishes, all the words will be in a single row, separated by commas:"]}
{"contents": ["Article extraction is the process of extracting data fields from an article page and putting it into a machine-readable structured format like JSON. In many use cases, the article page that you want to extract is a news page but it can be any other type of article. Based on our experience in the web data extraction industry for over 10 years, the demand for structured article data is getting higher. There is more information available on the internet than ever. But still, having access to structured news data and being able to consume relevant and timely information can set you apart and give you a competitive edge. This is what article extraction can do for you.", "In order to extract article data from the web, you need an extraction tool. It can be a challenge to find the tool that is best suited to meet your needs and provides the functionality and data quality that you expect. In this article, we discuss the most used tools for article extraction that you can choose from.", "We\u2019re going to look at both open source and commercial solutions. Hopefully, by the end of this blog post, you will have a better understanding of the available article extraction tools and you will be able to make an educated decision whether an open-source or closed-source tool is the best for you to extract news and/or articles.", "Before looking at the tools, let\u2019s have a quick overview of what is included in article extraction and why you would extract articles.", "An article\u2019s most important data field and many times the reason you want to extract the data is the ", " field, which contains the text of the article. But other than the text body, there are many other fields you can and might want to extract.", "At Scrapinghub, we\u2019ve seen numerous web data extraction projects that included article extraction.", "As you can see, article data can provide the most value when building on top of it. But in order to achieve this, you need a reliable way to get structured article data feeds.", "An important requirement for any article extraction tool is that it needs to work for most websites without writing any site-specific code. The reason for this is that writing custom rule-based extraction code requires a lot of maintenance, especially if you\u2019re extracting data from hundreds or thousands of domains.", "Now let\u2019s look at the available tools!"]}
{"contents": ["Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions.", "However, as the number of websites continues to grow, data extraction has become increasingly complex. Constantly changing website structures, antibot practices and scaling requirements add to the difficulty.", "To overcome the ever-increasing challenges, the web data extraction industry has gone through impressive innovations and transformations in recent years that enable extracting accurate data in an efficient manner. During this revolutionary time in the industry, we are delighted to announce that Scrapinghub will be once again hosting the", " this year on ", ".", "This year, however, things will be a little different. Given ", "Inspired by the great success of some of the talks during the Web Data Extraction Summit 2019 and feedback from attendees, we are planning a ", " this year. Sessions will include talks ranging from basic approaches to scrape to advanced level use of AI and machine learning in data extraction. Along with use cases on how companies from different industries use web extracted data and follow-up sessions on some interesting topics covered last year such as tackling antibots. Our Data Scientists and Product Engineers will also give you a sneak peek of some of the most cutting edge technologies in the web data extraction industry.\u00a0", "Don\u2019t miss out on the chance to be amidst hundreds of data lovers and hear from some of the leaders and pioneers of the web scraping and data extraction industry.", "Oh, and did we mention it's completely free!\u00a0"]}
{"contents": ["During a broad crawl, you might be extracting data from thousands or tens of thousands of websites with different layouts. When you scrape this many websites using a single spider, analyzing and validating the extracted data can be challenging. One important question to answer is what criteria should be used to determine the overall quality of the dataset.", "In this article, which is the final part of our QA series, we\u2019re going to work with 20 000 different sites and go through all steps with detailed explanations about the process. In this example, we are interested in finding specific keywords on the sites - if the keyword is found on a page then it should be flagged for further analysis.", "A large subset of heterogeneous sites should be processed to collect information for a specific keyword. We are going to crawl recursively starting from the domain and check all found links on the landing page. Several types of limits can be introduced here, like:", "The goal is to get as many pages as possible and verify the existence of keywords in them.", "The first step is inspired by two popular topics in programming:", "S", "The idea is to identify good representations of 1-5% of the dataset and work with it. So for 20,000 sites, we can start with 200 sites that represent best the dataset. A random selection might work as well when it\u2019s not possible to pick by other methods.", "At this point, these should be clear:", "Let's assume that our sites are named like this:", "We are going to use a semi-automated approach to verify the first"]}
